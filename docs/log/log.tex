\documentclass[a4paper]{article}

\usepackage[a4paper, margin=0.5in]{geometry}
\usepackage{graphicx}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}
\usepackage{color,soul}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{textcomp}
\usepackage{amsmath, amssymb}
\usepackage{caption}
\usepackage{listings}
\usepackage[italian]{babel}

% figure support
\usepackage{tikz}
\usetikzlibrary{calc}
\usepackage{import}
\usepackage{xifthen}
\pdfminorversion=7
\usepackage{pdfpages}
\usepackage{transparent}
\usepackage[hidelinks]{hyperref}
\usepackage{multirow}

% provides the H option
\usepackage{float}

\pdfsuppresswarningpagegroup=1

\begin{document}
	\title{Project log - Robotica}
	\author{Augello Andrea \and Castiglione Francesco Paolo \and La Martina Marco}
	\maketitle
	\tableofcontents

	\section{Setup}\label{sec:Setup}
	\begin{tabular}{|l|r|}
		\hline
		\multirow{2}{4em}{OS} & Ubuntu 18.04 \\
							  & Ubuntu 20.04 \\ \hline
		\multirow{2}{6em}{ROS version} & melodic \\
									   & noetic \\ \hline
		Webots & R2020b revision 1\\ \hline
		\multirow{2}{11em}{Target hardware} & Raspberry Pi 4B \\
											& Raspberry Pi 3B+ \\ \hline
	\end{tabular}

	\section{Nome}\label{sec:Nome}
	Il team ha scelto il nome \textbf{Change} in onore di \textbf{Chang'e 4} \cite{change4}, la missione parte della seconda fase del programma cinese di esplorazione lunare, durante il quale è andato a buon fine il primo atterraggio morbido sulla faccia nascosta della luna. 

	\section{Ambiente}\label{sec:Ambiente}
	Abbiamo considerato opportuno analizzare e studiare il package \textbf{webots\_ros} \cite{webotsRos} al fine di raggiungere una comprensione più profonda sulle metodologie per interfacciare i nodi ROS con il controller ROS standard per Webots. Inoltre è risultato necessario approfondire la documentazione ROS \cite{rosTutorial} al fine di installare e configurare l'ambiente ROS ed inoltre per capire i concetti fondamentali relativi ai nodi e topics. Infine abbiamo impostato l'interfaccia ROS su Webots seguendo la documentazione cyberbotics rilevante \cite{rosTutorial}.
	
	\section{Dipendenze}\label{sec:Dipendenze} 
	La seguente è una lista delle librerie utilizzate nel nostro progetto ed una breve spiegazione della loro funzione e rilevanza:
	
	\begin{itemize}
		\item opencv 4.x, una libreria per la computer vision, usata per operazioni di segmentazione \cite{opencv};
		\item imutils, che include funzioni per semplici operazioni di image processing quali traslazioni, rotazioni, ridimensionamento. Utilizzato inoltre per effettuare Non Maxima Suppression(NMS) \cite{imutils};
		\item sklearn, una libreria per il machine learning comprendente algoritmi di clustering quali DBSCAN \cite{scikit};
		\item numpy, una libreria che fornisce supporto per array multidimensionali, matrici ed operazioni matematiche per lavorare su detti array \cite{numpy};
		\item matplotlib, una libreria per creare visualizzazioni di dati (statiche, dinamiche, interattive) \cite{matplotlib};
		\item math, una libreria che fornisce funzioni matematiche definite dallo standard C \cite{math}.
	\end{itemize}

	\section{Obbiettivo}\label{sec:Obbiettivo} 
	L'obbiettivo del robot è di \textbf{evitare assembramenti in ambienti indoor}. \newline
	Nella dimostrazione presentata il nostro robot rileva le persone nella stanza e individua i possibili assembramenti. In seguito alla fase di rilevazione si sposterà verso l'assembramento evitando gli ostacoli e, arrivato, esorterà le persone al rispetto del distanziamento sociale.
	
	\section{TIAGo Iron}\label{sec:TIAGo-Iron} 
	Il robot scelto per l'obbiettivo proposto è il \textbf{TIAGo Iron}. \newline Il \textbf{PAL Robotics TIAGo Iron} \cite{tiagoiron} è un robot umanoide a due ruote con torso e testa ma senza braccia articolate. Il modello è una piattaforma modulare mobile che permette l'interazione fra esseri umani e robot. \newline
	Abbiamo ritenuto necessario per il nostro scopo aggiungere uno \textbf{speaker} e un \textbf{display} con corrispondente solido di supporto al modello base del \textbf{TIAGo} disponibile in Webots.
	Inolte è stato necessario contattare gli sviluppatori di Webots per chiedere informazioni circa le dimensioni esatte delle \textbf{ruote} in quanto il modello Webots non corrisponde perfettamente alle specifiche indicate nel datasheet \cite{Tiago IRON datasheet}. Le ruote del modello hanno raggio di 200mm. 
	
    L'IMU utilizzata ha 6 gradi di libertà ed è composta delle seguenti componenti:
	\begin{enumerate}
		\item giroscopio;	
		\item accelerometro;
	\end{enumerate}
	Abbiamo ritenuto non necessario aggiungere il \textbf{magnetometro} in quanto in uno scenario reale sarebbe stato soggetto ad interferenze (significativamente più di un giroscopio), specialmente in un ambiente con molti oggetti metallici (quale potenzialmente lo scenario di utilizzo del nostro robot).

	\section{Modello del moto e posizionamento}\label{sec:Modello-del-moto-e-posizionamento}
	Il modello del moto è caratterizzato da rotazioni e traslazioni. Per ottenere l'angolo di rotazione ci basiamo sui dati ottenuti dal giroscopio, il quale misura il moto rotazionale fornendo una velocità angolare. Per ottenere l'angolo di rotazione effettuiamo quindi un'integrazione discreta dei campioni con interpolazione lineare del primo ordine.
	Per effettuare lo spostamento lineare utilizziamo il controllore PID (Proporzionale-Integrale-Derivativo) delle ruote fornito da Webots, che richiede l'angolo di rotazione corrente, il diametro delle ruote e fornisce l'angolo di rotazione necessario al fine di ottenere lo spostamento desiderato.
	\begin{equation}\label{eq:odometry}
	targetAngle =
	currentAngle+2\pi\frac    {distance}
	{2\pi \cdot diameter}
	\end{equation}
	
	A causa di possibili imprecisioni calcoliamo la stima dello spostamento lineare utilizzando l'accelerometro. Al segnale dell'accelerometro viene applicato un integrale doppio per ottenere lo spostamento lineare.
	
	\section{Object recognition}\label{sec:Object-recognition}
	Al fine di riconoscere le persone è stato necessario utilizzare sistemi di \textbf{object recognition}. A tal fine abbiamo valutato le performance di YOLOv3 (you only look once), YOLOv3-tiny, HoG (Histogram of oriented gradients), HoG + SVG (support vector machines) + NMS (non maxima suppression).
	In seguito a vari test su HoG abbiamo ritenuto essere problematica la larghezza delle bounding boxes fornite, in quanto, per motivazioni che verranno chiarite nel paragrafo successivo, vogliamo che queste ultime siano il più possibili vicile alla reale larghezza delle persone. YOLOv3, nonostante sia stato addestrato su foto di persone reali (e non modelli 3D) fornisce risultati soddisfacenti, in seguito al finetuning degli iperparaemtri parametri della rete. Tuttavia, considerando le caratterisiche hardware del robot mobile, abbiamo optato per l'uso di YOLOv3, il quale risulta essere significativamente più efficiente, sacrificando in termini di precisione ma comunque sufficientemente preciso per il nostro obbiettivo. Ecco un paragone fra YOLOv3 e YOLOv3-tiny in termini di mAP (mean average precision) e FLOPS (floating-point operations per second), come illustrato dalla tabella seguente, i cui dati provengono dal sito di YOLO \cite{yolo}:
	
	\begin{center}
		\begin{tabular}{ |c|c|c|c| } 
			\hline
			Model & mAP & FLOPS & FPS \\
			\hline	
			 YOLOv3-320    & 51.5  &  38.97  Bn  &  45  \\ 
			 YOLOv3-416    & 55.3  &  65.86  Bn  &  35  \\ 
			 YOLOv3-608    & 57.9  &  140.69 Bn  &  20  \\ 
			 YOLOv3-tiny   & 33.1  &  5.56   Bn  &  220 \\
			 YOLOv3-spp    & 60.6  &  141.45 Bn  &  20  \\
			\hline
		\end{tabular}
	\end{center}

	\section{Calcolo della distanza}\label{sec:Calcolo-della-distanza}
	
	L'altezza percepita dell'oggetto non è un indicatore affidabile della sua distanza dal robot in quanto parte dell'oggetto potrebbe essere occlusa o non presente nel frame. Inoltre classificatori quali HoG tendono a produrre ROI significativamente più alte dell'oggetto. La larghezza del torso, invece, è meno suscettibile a tali problemi, e non dipende dalla posizione (ad esempio seduto o alzato). Dobbiamo tuttavia ipotizzare che il torso abbia forma cilindrica introducendo quindi degli errori se l'obbiettivo non sta guardando la camera.	La posizione orizzontale dell'oggetto relativa alla camera può influenzare la larghezza percepita diminuendola quando aumenta la distanza dal centro dell'immagine. Ipotizzando che la camera abbia un FOV (field of view) di $2\alpha$ e sia distante $d$ dall'oggetto, la massima distanza orizzontale che un punto dell'immagine potrebbe avere dal centro del piano dell'immagine sarebbe $a = d \tan alpha$ (Fig.~\ref{fig:error}).
	
	\begin{figure}[htpb]
		\centering
		\includegraphics[width=0.8\textwidth]{./img/linearization_error.pdf}
		\caption{Errore nella stima della distanza con una linearizzazione a tratti della circonferenza}
		\label{fig:error}
	\end{figure}

	Ignorare la prospettiva significa effettuare un'approssimazione lineare del primo ordine e trattare il punto come se si trovasse su una circonferenza di raggio $d$ centrata sulla camera. Di conseguenza consideriamo il punto come se fosse più vicino di quanto non sia realmente, commettendo l'errore mostrato nell' Eq.~\ref{eq:max_err}. Con una camera con FOV di 1 radiante quale quella del TIAGo il massimo errore causato dalla linearizzazione è quindi una sottostima del 13.9\%.
	
	\begin{equation}
	\epsilon = 
	\sqrt{a^2+d^2} - d =
	\sqrt{(d\tan \theta )^2+d^2}-d =
	d\left( \sqrt{\frac{1}{\cos ^2 \alpha}}-1 \right) =
	d \left( \sec \alpha -1 \right) 
	\label{eq:max_err}
	\end{equation}
	
	Sotto tali ipotesi possiamo quindi calcolare la distanza di un oggetto come mostrato in
	Eq.~\ref{eq:obj_dist}
	
	\begin{equation}\label{eq:obj_dist}
	object~distance(m) = 
	\frac{f(m) \times real~width(m) \times image~width(pixels)}
	{object~width(pixels) \times sensor~width(m)}
	\end{equation}
	
	Poiché stiamo utilizzando un simulatore non è nota la larghezza del sensore da utilizzare per l'eq.~\ref{eq:obj_dist}. Abbiamo ovviato a tale problema posizionando il robot ed un oggetto dalle dimensioni note in posizioni note e abbiamo utilizzato questi dati insieme a delle misure in pixel nell' eq.~\ref{sensor_size}. Abbiamo così stimato le dimensioni del sensore virtuale da utilizzare nei calcoli successivi.

	\begin{equation}\label{sensor_size}
	sensor~width(m) = 
	\frac{f(m) \times real~width(m) \times image~width(pixels)}
	{object~width(pixels) \times object~distance(m)}
	\end{equation}
		
	\section{Clustering}
	We decided to lower the dimensionality of our data. We used cylindrical coordinates and the feature vector is 2 dimensional.
	We used the Density-Based Scan with a threshold. The entities not belonging to the cluster are discarded.
	
	\section{Projection Matrix}
	\cite{OpenGL}
	
	\section{ROS}
	
	\section{Bugs found in the Webots ROS Controller}
	Logical values did not allow callbacks.
	
	\newpage
	% Bibliography
	\bibliographystyle{unsrt}
	\begin{thebibliography}{19}
		\bibitem{tiagoiron} 
		\textit{https://cyberbotics.com/doc/guide/tiago-iron}. \newline
		Webots TIAGo Iron documentation.
		\bibitem{change4} 
		\textit{https://www.theguardian.com/science/2019/jan/03/china-probe-change-4-land-far-side-moon-basin-crater}. \newline
		The Guardian, 3 January 2019.
		\bibitem{webotsRos} 
		\textit{https://github.com/cyberbotics/webots\_ros}. \newline
		Github page for the \texttt{webots\_ros} package from \textit{cyberbotics}.
		\bibitem{rosTutorial} 
		\textit{https://wiki.ros.org/ROS/Tutorials}. \newline
		ROS documentation from ROS.org.
		\bibitem{webotsRosSetup} 
		\textit{https://www.cyberbotics.com/doc/guide/tutorial-8-using-ros}. \newline
		Cyberbotics documentation.
		\bibitem{Tiago IRON datasheet} 
		\textit{https://pal-robotics.com/wp-content/uploads/2019/07/Datasheet\_TIAGo\_Complete.pdf}. \newline
		Tiago IRON datasheet.
		\bibitem{OpenGL} 
		\textit{https://www.songho.ca/opengl/gl\_projectionmatrix.html}. \newline
		OpenGL Projection Matrix.
		\bibitem{positioning} 
		\textit{https://www.nxp.com/docs/en/application-note/AN3397.pdf}. \newline
		Implementing Positioning Algorithms Using Accelerometers.
		\bibitem{gmapping} 
		\textit{https://people.eecs.berkeley.edu/~pabbeel/cs287-fa11/slides/gmapping.pdf}. \newline
		Gmapping from UC Berkeley EECS, Pieter Abbeel.
		\bibitem{opencv} 
		\textit{https://opencv.org/}. \newline
		OpenCV Website.
		\bibitem{imutils} 
		\textit{https://github.com/jrosebr1/imutils}. \newline
		Imutils GitHub page.
		\bibitem{scikit} 
		\textit{https://scikit-learn.org/stable/}. \newline
		Scikit-learn website.
		\bibitem{numpy} 
		\textit{https://numpy.org/}. \newline
		Numpy website.
		\bibitem{matplotlib} 
		\textit{https://matplotlib.org/}. \newline
		Matplotlib website.
		\bibitem{math} 
		\textit{https://docs.python.org/3/library/math.html}. \newline
		Python documentation.
		\bibitem{yolo} 
		\textit{https://pjreddie.com/darknet/yolo/}. \newline
		Yolo website.
	\end{thebibliography}

\end{document}
